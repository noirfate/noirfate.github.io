---
title: Build Kubernets Debug Environment
layout: post
categories: k8s
tags: k8s cloud
date: 2022-04-18 18:00
excerpt: Build Kubernets Debug Environment
---

{:.table-of-content}
* TOC
{:toc}

# 搭建k8s调试环境
## 单机搭建k8s集群
1. 下载[metarget](https://github.com/Metarget/metarget)工具
2. 进入metarget目录，把以下内容保存到metarget.patch，执行`patch -p1 < metarget.patch`
```
--- metarget-master/core/env_managers/kubernetes_installer.py   2022-04-06 14:19:27.000000000 +0800
+++ metarget/core/env_managers/kubernetes_installer.py  2022-04-16 09:27:17.333111148 +0800
@@ -34,7 +34,7 @@
     _cmd_kubeadm_list_image = 'kubeadm config images list'.split()
     _cmd_kubeadm_reset = 'kubeadm reset'.split()
     _cmd_enable_schedule_master = 'kubectl taint nodes --all node-role.kubernetes.io/master-'.split()
-    _kubeadm_common_options = '--ignore-preflight-errors=NumCPU,cri'
+    _kubeadm_common_options = '--ignore-preflight-errors=NumCPU,cri,SystemVerification,Service-Docker'

     @classmethod
     def uninstall(cls, verbose=False):
```
3. 重新把metarget打包为metarget.tar.gz
4. 准备`sources.list`
```
deb http://mirrors.aliyun.com/ubuntu/ bionic main restricted
deb-src http://mirrors.aliyun.com/ubuntu/ bionic main restricted
deb http://mirrors.aliyun.com/ubuntu/ bionic-updates main restricted
deb-src http://mirrors.aliyun.com/ubuntu/ bionic-updates main restricted
deb http://mirrors.aliyun.com/ubuntu/ bionic universe
deb-src http://mirrors.aliyun.com/ubuntu/ bionic universe
deb http://mirrors.aliyun.com/ubuntu/ bionic-updates universe
deb-src http://mirrors.aliyun.com/ubuntu/ bionic-updates universe
deb http://mirrors.aliyun.com/ubuntu/ bionic multiverse
deb-src http://mirrors.aliyun.com/ubuntu/ bionic multiverse
deb http://mirrors.aliyun.com/ubuntu/ bionic-updates multiverse
deb-src http://mirrors.aliyun.com/ubuntu/ bionic-updates multiverse
deb http://mirrors.aliyun.com/ubuntu/ bionic-backports main restricted universe multiverse
deb-src http://mirrors.aliyun.com/ubuntu/ bionic-backports main restricted universe multiverse
deb http://mirrors.aliyun.com/ubuntu bionic-security main restricted
deb-src http://mirrors.aliyun.com/ubuntu bionic-security main restricted
deb http://mirrors.aliyun.com/ubuntu bionic-security universe
deb-src http://mirrors.aliyun.com/ubuntu bionic-security universe
deb http://mirrors.aliyun.com/ubuntu bionic-security multiverse
deb-src http://mirrors.aliyun.com/ubuntu bionic-security multiverse
```
5. 准备dockerd配置文件，配置存储引擎为vfs，`"storage-driver":"vfs"`
6. 创建Dockerfile
```
FROM ubuntu:bionic
WORKDIR /root
COPY sources.list /etc/apt/
ADD metarget.tar.gz /root/
RUN apt update && apt install -y vim python3 python3-pip unzip kmod docker.io software-properties-common systemd net-tools gdb
COPY daemon.json /etc/docker/
RUN pip3 install -r ./metarget/requirements.txt

ENTRYPOINT ["/sbin/init"]
```
7. 构建容器`docker build -t metarget .`
8. 运行容器
```
docker run --privileged --name master -v /lib/modules:/lib/modules -v /etc/localtime:/etc/localtime -idt metarget
```
9. 进入容器，利用`metarget`安装指定版本的k8s集群，以`1.19.10`为例，如果想让`master`也能部署容器，在命令的最后添加`--taint-master`
```
cd metarget
./metarget gadget install k8s -v 1.19.10 --domestic
```
10. 执行`kubectl get nodes`验证master节点已安装完毕
![](/assets/img/k8s_env1.png)
11. 拷贝`tools/install_k8s_worker.sh`到宿主机
12. 创建node节点容器
```
docker run --privileged --name node1 -v /lib/modules:/lib/modules -v /etc/localtime:/etc/localtime -idt metarget
```
13. 拷贝`tools/install_k8s_worker.sh`到node节点容器中并执行`bash install_k8s_worker.sh`
![](/assets/img/k8s_env2.png)
14. 在master节点上查看新添加的node
![](/assets/img/k8s_env3.png)
15. 测试部署`pod`
![](/assets/img/k8s_env4.png)
![](/assets/img/k8s_env5.png)

## 安装k8s调试环境
1. 安装golang
```
wget https://dl.google.com/go/go1.18.1.linux-amd64.tar.gz
tar -C /usr/local -xzf go1.18.1.linux-amd64.tar.gz
ln -s /usr/local/go/bin/go /usr/bin/go
ln -s /usr/local/go/bin/gofmt /usr/bin/gofmt
go env -w GOPROXY="https://mirrors.aliyun.com/goproxy/,direct"
```
2. 下载目标版本`https://github.com/kubernetes/kubernetes/releases`源码
3. 编译
```
apt install -y rsync
# go tool compile --help
# -N disable optimizations
# -l disable inlining
# Specify GOLDFLAGS as an empty string for building unstripped binaries 
make all GOGCFLAGS="-N -l" GOLDFLAGS=""
# if error then chmod +x _output/bin/prerelease-lifecycle-gen
```
4. 安装etcd，`./hack/install-etcd.sh`，会从github上下载
5. 运行`./hack/local-up-cluster.sh -O`，启动单机版k8s
6. 为gdb添加go语言支持`echo "add-auto-load-safe-path /usr/local/go/src/runtime/runtime-gdb.py" >> ~/.gdbinit`
7. 用gdb进行调试

## 制作k8s调试镜像
1. 安装go、下载源码（见`安装k8s调试环境`）
2. 编译镜像
	- 编译镜像时会下载kube-cross，参见`build/build-image/Dockerfile`，由于k8s编译对go版本有要求，需要`1.15`以上的版本，但国内貌似只能下到`registry.aliyuncs.com/google_containers/kube-cross:v1.12.10-1`，无法使用
	- 可修改`build/build-image/cross/VERSION`，在编译时下载指定版本的`kube-cross`镜像
	- 编译时需要占用大量磁盘空间，建议预留70G
	- 可修改`build/common.sh`中的`debian_iptables_version`和`go_runner_version`为对应版本，此处分别是`bullseye-v1.3.0`和`v2.3.1-go1.18.1-bullseye.0`
```
docker pull noirfate/kube-cross:v1.16.7-1
docker tag noirfate/kube-cross:v1.16.7-1 k8s.gcr.io/build-image/kube-cross:v1.16.7-1
docker rmi noirfate/kube-cross:v1.16.7-1
echo "v1.16.7-1" > build/build-image/cross/VERSION

docker pull noirfate/go-runner:v2.3.1-go1.18.1-bullseye.0
docker tag noirfate/go-runner:v2.3.1-go1.18.1-bullseye.0  k8s.gcr.io/build-image/go-runner:v2.3.1-go1.18.1-bullseye.0
docker rmi noirfate/go-runner:v2.3.1-go1.18.1-bullseye.0

docker pull noirfate/debian-iptables:bullseye-v1.3.0
docker tag noirfate/debian-iptables:bullseye-v1.3.0 k8s.gcr.io/build-image/debian-iptables:bullseye-v1.3.0
docker rmi noirfate/debian-iptables:bullseye-v1.3.0

KUBE_BUILD_PLATFORMS=linux/amd64 KUBE_BUILD_CONFORMANCE=n KUBE_BUILD_HYPERKUBE=n KUBE_BUILD_PULL_LATEST_IMAGES=n make quick-release-images GOGCFLAGS="-N -l" GOLDFLAGS=""
```
3. 编译完成后会生成镜像tar包
![](/assets/img/k8s_env6.png)
4. 运行`单机搭建k8s集群`生成的镜像，加载tar包
![](/assets/img/k8s_env7.png)
5. 重新打tag，把`-amd64`去掉
```
docker tag k8s.gcr.io/kube-proxy-amd64:v1.19.10 k8s.gcr.io/kube-proxy:v1.19.10
docker rmi k8s.gcr.io/kube-proxy-amd64:v1.19.10
docker tag k8s.gcr.io/kube-apiserver-amd64:v1.19.10 k8s.gcr.io/kube-apiserver:v1.19.10
docker rmi k8s.gcr.io/kube-apiserver-amd64:v1.19.10
docker tag k8s.gcr.io/kube-controller-manager-amd64:v1.19.10 k8s.gcr.io/kube-controller-manager:v1.19.10
docker rmi k8s.gcr.io/kube-controller-manager-amd64:v1.19.10
docker tag k8s.gcr.io/kube-scheduler-amd64:v1.19.10 k8s.gcr.io/kube-scheduler:v1.19.10
docker rmi k8s.gcr.io/kube-scheduler-amd64:v1.19.10
```
6. 运行`metarget`安装集群
```
./metarget gadget install k8s -v 1.19.10 --domestic --taint-master
```

## 调试k8s
1. 合并`单机搭建k8s集群`和`安装k8s调试环境`
```
FROM ubuntu:bionic
WORKDIR /root
COPY sources.list /etc/apt/
ADD metarget.tar.gz /root/
RUN apt update && apt install -y vim python3 python3-pip unzip kmod docker.io software-properties-common systemd net-tools gdb rsync autotools-dev automake libncurses5-dev texinfo flex libreadline-dev
COPY daemon.json /etc/docker/
RUN pip3 install -r ./metarget/requirements.txt
COPY go1.18.1.linux-amd64.tar.gz /root
RUN tar -C /usr/local -xzf go1.18.1.linux-amd64.tar.gz && ln -s /usr/local/go/bin/go /usr/bin/go && ln -s /usr/local/go/bin/gofmt /usr/bin/gofmt && go env -w GOPROXY="https://mirrors.aliyun.com/goproxy/,direct" && rm -f go1.18.1.linux-amd64.tar.gz && echo "add-auto-load-safe-path /usr/local/go/src/runtime/runtime-gdb.py" >> ~/.gdbinit
ADD kubernetes-1.19.10.tar.gz /root
#RUN cd kubernetes-1.19.10 && make all GOGCFLAGS="-N -l" GOLDFLAGS=""
ENTRYPOINT ["/sbin/init"]
```
2. 构建镜像
```
docker build -t metarget_dbg .
```
3. 运行镜像
```
docker run --name master --privileged -v /lib/modules:/lib/modules -v /etc/localtime:/etc/localtime -idt metarget_dbg
```
4. 编译k8s
```
make all GOGCFLAGS="-N -l" GOLDFLAGS=""
```
5. 安装集群
```
./metarget gadget install k8s -v 1.19.10 --domestic --taint-master
```
6. 停止`kube-apiserver`
```
ps -efww | grep kube-apiserver
mv /etc/kubernetes/manifests/kube-apiserver.yaml /etc/kubernetes/manifests/kube-apiserver.yaml.bak
docker stop [kube-apiserver容器id]
```
7. 马上手动启动自己编译的`kube-apiserver`，如果`kube-apiserver`已经重启了，则再用`docker stop`停掉它
```
./_output/bin/kube-apiserver --advertise-address=172.17.0.2 --allow-privileged=true --authorization-mode=Node,RBAC --client-ca-file=/etc/kubernetes/pki/ca.crt --enable-admission-plugins=NodeRestriction --enable-bootstrap-token-auth=true --etcd-cafile=/etc/kubernetes/pki/etcd/ca.crt --etcd-certfile=/etc/kubernetes/pki/apiserver-etcd-client.crt --etcd-keyfile=/etc/kubernetes/pki/apiserver-etcd-client.key --etcd-servers=https://127.0.0.1:2379 --insecure-port=0 --kubelet-client-certificate=/etc/kubernetes/pki/apiserver-kubelet-client.crt --kubelet-client-key=/etc/kubernetes/pki/apiserver-kubelet-client.key --kubelet-preferred-address-types=InternalIP,ExternalIP,Hostname --proxy-client-cert-file=/etc/kubernetes/pki/front-proxy-client.crt --proxy-client-key-file=/etc/kubernetes/pki/front-proxy-client.key --requestheader-allowed-names=front-proxy-client --requestheader-client-ca-file=/etc/kubernetes/pki/front-proxy-ca.crt --requestheader-extra-headers-prefix=X-Remote-Extra- --requestheader-group-headers=X-Remote-Group --requestheader-username-headers=X-Remote-User --secure-port=6443 --service-account-key-file=/etc/kubernetes/pki/sa.pub --service-cluster-ip-range=10.96.0.0/12 --tls-cert-file=/etc/kubernetes/pki/apiserver.crt --tls-private-key-file=/etc/kubernetes/pki/apiserver.key
```
8. 使用gdb调试，gdb显示源码比较乱，可以用cgdb(`https://github.com/cgdb/cgdb`)
9. 恢复`kube-apiserver`
	- 恢复`kube-apiserver.yaml`
	- 如果存在`kube-apiserver`pod的`pause`容器则停掉
	- 等待一会儿，如果`kube-apiserver`没有起来，杀掉`kubelet`进程（会自动重启）