---
title: What's AI Red-teaming
layout: post
categories: AI
tags: security
date: 2024-08-29 18:00
excerpt: What's AI Red-teaming
---

{:.table-of-content}
* TOC
{:toc}

# 何为AI红队

## 什么是红队演练（Red Teaming）
### 历史
“红队”这个术语起源于冷战时期的美国军队，可以追溯到1960s，它源于战争博弈的博弈论方法，以及由兰德公司开发并由五角大楼的政策分析家应用于评估战略决策的模拟行动。“红色”指代苏联，更广泛地指任何对手或对抗立场。在典型的演习中，蓝队（美国）将防御红队（苏联）。

- 1963年5月
专栏作家乔治·狄克逊(George Dixon)在一篇文章中讨论了国防部长罗伯特·麦克纳马拉(Robert McNamara)如何组建了一个蓝队和一个红队来评估价值65亿美元的TFX(战术战斗机实验)飞机合同的授予。
- 1963年9月
《冲突解决杂志》(Journal of Conflict Resolution)的一篇文章描述了两年前的一个类似模拟。这个模拟以美国蓝队和模仿苏联的"红队"之间的结构化军备控制游戏为特色。目标是研究领导人的决策,并确定其对军备控制条约条款的影响。
- 2003年9月
美国国防科学委员会(DSB)发布了一份关于"国防部红队活动的角色和地位"的报告，指出高级官员的关注是必要的，并建议时任国防部长唐纳德·拉姆斯菲尔德(Donald Rumsfeld)通过发布指导备忘录，将红队的实践推广到整个部门，并将红队工作纳入教育机构和活动中。报告还建议国防部长为关键问题建立红队，例如维护核武器库存和寻找难以捉摸的恐怖分子头目。为此，美国军方对红队做了标准定义：“由训练有素、受过教育和经验丰富的专家组成的团队，为联合部队指挥官提供独立的能力，以进行关键的审查和分析、探索计划和行动，并从另一个角度分析对手的能力。”
- 2004年4月
在陆军教育系统内建立所谓的红队大学，即外国军事与文化研究大学（UFMCS），退役陆军上校格雷戈里·方特诺（Gregory Fontenot）被任命为UFMCS的主任，其目标是“永久性地改变陆军未来领导者的思维方式”。[The Red Team Handbook](https://usacac.army.mil/sites/default/files/documents/ufmcs/The_Red_Team_Handbook.pdf)
- 2015年4月
CNSS（Committee on National Security Systems 国家安全系统委员会）在其发布的文件`CNSSI 4009-2015`术语表中增加了对`red team`的定义，将其用于网络空间

```
A group of people authorized and organized to emulate a potential adversary’s attack or exploitation capabilities against an enterprise’s security posture. The Red Team’s objective is to improve enterprise cybersecurity by demonstrating the impacts of successful attacks and by demonstrating what works for the defenders (i.e., the Blue Team) in an operational environment. Also known as Cyber Red Team.

红队是一组经过授权和组织的人员，他们的任务是模拟潜在对手的攻击或渗透能力，以评估企业的安全状况。红队的目标是通过演示成功攻击的影响，以及在实际运行环境中展示防守方（即蓝队）的有效防御措施，来提高企业的网络安全水平。红队也被称为网络红队。
```

### 最佳实践
#### 老板必须认可
无论老板是军事指挥官、政府机构官员、首席信息官还是高级副总裁，负责人必须重视红队。同样重要的是，向所有相关员工表明他们的支持。用军事术语来说,这种"高层支持"非常关键。正如退休海军陆战队中将、广受认可的红队大师Paul Van Riper所宣称的那样：“除非指挥官自己想要它，支持它，为它提供资源，将其制度化，并对它做出回应，否则它不会有任何意义。”

- 老板必须意识到组织内部存在红队可以帮助发现和解决的漏洞。组织往往不善于判断自己的绩效，常常对不足和陷阱视而不见。事实上，在许多情况下，必须已经发生了明显的失败或灾难——导致了重大的人员、财务或声誉成本——老板才会愿意听取红队的呼吁。例如，直到泛美103航班爆炸事件发生后(270人丧生)，联邦航空管理局（FAA）的管理者才建立了一个小型红队，以进行实际的威胁和漏洞评估。

- 老板必须愿意投入资源、人员和时间来支持内部或外部红队以审查他们的机构。红队很少是组织核心任务的关键活动。因此，它可能面临资金障碍，而且往往在其需求不明显时就被削减。

- 老板必须允许红队成员对他们的发现完全诚实，不能因为揭示问题而受到惩罚

- 进行红队测试的领导应该有权力授权实施红队的测试结果，或强烈建议更高层的老板予以认可，否则之后的改进则无法推进

#### 外部客观、内部知情
红队必须平衡几个相互竞争的原则：在保持独立和客观的同时，对组织的运作环境及其可用资源保持敏感。

- 定位
理想情况下，红队应该通过一条直接通向它所负责的最高领导的虚线汇报，在一定程度上独立于等级制度。

- 范围
在开始工作之前，红队必须与目标机构明确达成共识，确切地了解谁或什么将成为红队的目标，持续多长时间，具有什么程度的灵活性，以及最终目的是什么。

- 知情
红队需要对目标机构有一个扎实的理解，例如了解因为什么担忧或缺陷促使目标机构进行红队测试

- 安全
红队测试不应该对目标机构造成意外的干扰或损害

#### 具备技巧的无畏怀疑者
如果红队的组建是随意的，随便找一些当时可用的人，而不考虑他们的技能或性格类型，那么几乎可以肯定会失败。红队需要在思维或行动上与目标机构中的其他人有所不同。

- 反应迅速，适应能力强，自我激励，并在追求他们认为真实的事物时无所畏惧，同时也天生好奇，愿意倾听和向他人学习
- 可靠的教育和专业经验
- 彼此间能够进行良好的合作
- 红队从业者必须是多元化和具备开阔思维的。红队所应用的方法和技术不能变得例行化和可预见，否则红队就会简单地被纳入机构的正常计划和流程中。为了避免可预测性和机构的控制，优秀的红队成员必须拥有一个庞大且经过磨练的工具箱。

#### 愿意听取坏消息并采取行动
红队的调查结果和建议不能只是束之高阁。它们必须被倾听、采取行动并在老板认为可能的最大范围内付诸实施。

每个红队成员都可以讲述高级副总裁、三星上将或首席安全官在缺陷被提请注意后仍然固执地否认其存在的故事。一位白帽黑客转述说，他的团队在一家财富100强科技公司发现了相同的网络控制访问漏洞，持续了十多年。每次渗透测试后，都会向该公司的网络安全公司报告相同的漏洞，并提供一份如何以有限的成本修复或缓解该漏洞的计划。尽管一再警告，该公司从未解决这个备受关注的缺陷。

#### 适度进行红队测试，不要过度

如果红队行动过于频繁，机构将永远没有足够的时间来实施红队最近的发现和建议。正如詹姆斯·N·米勒(James N. Miller)所说，他在五角大楼和私营部门接触并领导了红队工作：“你不能把所有事情都红队至死,否则你将一事无成。但对于重要的项目，获得独立的眼光是非常宝贵的。”

此外，红队频繁评估或测试机构的日常计划和流程，可能会很快使员工士气低落。对于目标机构内受影响的员工来说，红队可能是一个高度压力的事件。人们往往对被测试，或在同事或老板面前质疑他们的判断持负面反应。另一方面，如果红队过于不频繁，机构就会变得固步自封和自满。一个机构应该多久使用一次红队技术，在很大程度上取决于其运营环境的静态或动态程度。一个面临相对较少新威胁或挑战的机构，不需要经常这样做。而面临高度竞争环境、不确定性和潜在灾难性威胁的机构应该更频繁地进行红队测试。

#### 红队演练结果

- 提供新的发现或见解，这些发现或见解是目标机构内部无法自行产生的
机构往往都面临着不同程度的结构性或文化性限制，而有效的红队工作可以克服这些限制。几乎每个领导者都声称重视开放性和创造力，以创造出为其机构增值的新想法和新概念。然而，使机构顺利运作所需的过程——如等级制度、正式规则、单位凝聚力和行为规范——恰恰是那些使差异化和多样化思维极难实现的过程。
- 揭示了目标机构的思维过程和价值观

## AI红队
### 定义
随着生成式AI的广泛应用，对它安全性的担忧也日益增长，为了应对这一状况，实践者和政策制定者们都将红队视为其识别和解决相关风险的战略中不可或缺的一部分，旨在确保与人类和社会价值观的一定程度的对齐。值得注意的是，美国总统拜登2023年10月在关于[安全、可靠和可信的人工智能开发与使用的行政命令](https://www.whitehouse.gov/briefing-room/presidential-actions/2023/10/30/executive-order-on-the-safe-secure-and-trustworthy-development-and-use-of-artificial-intelligence/)中提到红队八次，并对其进行了如下定义：

```
The term “AI red-teaming” means a structured testing effort to find flaws and vulnerabilities in an AI system, often in a controlled environment and in collaboration with developers of AI.  Artificial Intelligence red-teaming is most often performed by dedicated “red teams” that adopt adversarial methods to identify flaws and vulnerabilities, such as harmful or discriminatory outputs from an AI system, unforeseen or undesirable system behaviors, limitations, or potential risks associated with the misuse of the system.

“AI红队”是指一种结构化的测试工作，旨在发现AI系统中的缺陷和漏洞，通常在受控环境中进行，并与AI开发人员合作。人工智能红队工作通常由专门的"红队"执行，他们采用对抗性方法来识别缺陷和漏洞，例如AI系统产生的有害或歧视性输出、不可预见或不希望出现的系统行为、局限性或与系统滥用相关的潜在风险。
```

### 行业实践
#### [Google](https://services.google.com/fh/files/blogs/google_ai_red_team_digital_final.pdf)
##### 定义
为了应对潜在的挑战，谷歌创建了一个专门的人工智能红队。它与传统的红队密切相关，但也具备必要的人工智能专业知识，以对人工智能系统进行复杂的技术攻击。

红队模拟针对人工智能部署的威胁行为者
- 评估模拟攻击对用户和产品的影响，并确定增强和抵御这些攻击的方法
- 分析内置于核心系统的新人工智能检测和预防能力的韧性，并探讨攻击者可能如何绕过它们
- 利用红队结果改善检测能力，以便早期发现攻击，事件响应团队可以适当地做出反应。红队演练还为防御团队提供了一个机会，练习他们如何处理真实攻击
- 提高相关利益相关者的意识：1）帮助使用人工智能的开发者理解关键风险；2）根据需要，倡导组织基于风险驱动和充分知情的原则，对安全控制措施进行投资
<br>

##### 技战术
以下技战术是AI红队所特有的，需要与传统红队的技战术结合使用，而不是替代它们。
- 提示攻击
- 训练数据提取（如PII或密码等机密信息）
- 模型后门
- 对抗样本
- 数据投毒
- 模型萃取

##### 经验
- 传统红队是用来构建AI红队的一个很好的起点，但随着AI系统攻击的日趋复杂，与AI专家合作会得到很多益处
- 解决红队的发现可能具有挑战性，一些攻击可能没有简单的解决方案
- 针对许多攻击，传统的安全防护措施可以显著降低风险，在保护AI整个生命周期中的完整性，防止数据投毒和后门攻击中尤其如此

#### [Microsoft](https://www.microsoft.com/en-us/security/blog/2023/08/07/microsoft-ai-red-team-building-future-of-safer-ai/)

##### 定义
随着AI系统变得更加普遍，微软于2018年就成立了AI红队，它是一个由跨学科专家组成的团队，致力于像攻击者一样思考并探测AI系统的故障。如今AI红队的实践已经演变为一个更广泛的含义：它不仅涵盖了对安全漏洞的探测,还包括了对其他系统故障的探测，例如生成有害内容。AI系统带来了新的风险，而红队是了解这些新风险（如提示注入和生成有害内容）的核心。AI红队在微软不仅仅是锦上添花，它是负责任AI设计的基石。
![](/assets/img/llm_sec/ms_redteam.png)

##### 技战术
AI红队测试通常在以下两个层面进行：
- 模型层面
对模型进行红队测试有助于在过程的早期识别模型如何被滥用，确定模型的能力范围，并了解模型的局限性。这些见解可以反馈到模型开发过程中，以改进未来的模型版本，同时也可以快速确定模型最适合哪些应用程序
- 应用层面
在此层面上，AI红队测试采用系统视角，其中基础模型只是整个系统中的一部分，有助于识别超出模型级安全机制的问题，如系统漏洞、应用漏洞等
<br>

##### 经验
GenAI红队测试是一个复杂的多步骤过程，与传统AI系统或传统软件的红队测试有很大不同。
- 负责任AI
传统软件或经典AI的红队测试主要集中在识别安全故障上，而GenAI红队测试必须考虑负责任AI的风险。这些风险可能差异很大，从生成存在公平性问题的内容到产生没有根据或不准确的信息等等。GenAI红队测试必须同时探索潜在的安全风险和负责任AI故障。
- 概率性
GenAI红队测试比传统红队测试更具有概率性。在传统软件系统上多次执行相同的攻击路径会产生相似的结果，然而GenAI可以为相同的输入提供不同的输出。这可能是由于应用程序特定的逻辑或GenAI模型本身造成的。与具有明确定义的API和参数的传统软件系统不同，红队在评估GenAI系统时必须考虑其概率性质。
- 架构差异
不同类型的GenAI工具之间的系统架构差异很大。有独立的应用程序、有与现有应用程序的集成，以及红队需要考虑多种不同的输入和输出模式，如文本、音频、图像和视频。这些差异使得进行手动红队测试变得困难。例如，为了在基于浏览器的聊天界面上发现暴力内容生成风险，红队需要多次尝试不同的策略，以收集潜在失败的充分证据。对所有类型的危害、所有模态和策略手动执行此操作可能非常繁琐和缓慢。

#### [Hugging Face](https://huggingface.co/blog/zh/red-teaming)
##### 定义
红队是一种用于引出模型不良行为漏洞的评估形式，越狱是另一个红队术语，用来表示突破大语言模型的安全限制
##### 技战术
制作一个提示 (prompt)，该提示会触发模型生成有害内容

#### [Nvidia](https://developer.nvidia.com/blog/nvidia-ai-red-team-an-introduction/)
##### 定义
AI红队是一个由攻击安全专业人员和数据科学家组成的跨职能团队，利用综合技能从信息安全的角度评估AI系统，以识别和帮助缓解任何风险。
##### 技战术
这个框架使我们能够解决AI流水线、基础设施或技术中特定部分的特定问题。它成为一个与受影响系统沟通问题风险的地方：自上而下贯穿整个技术栈，并为政策和技术提供信息。
![](/assets/img/llm_sec/ai-red-team-assessment-framework.png)

- 风险
    - 技术风险：由于技术漏洞或缺陷，AI系统或流程受到损害
    - 声誉风险：模型的性能或行为对组织造成负面影响
    - 合规风险：AI系统不合规，导致罚款或市场竞争力下降
- 框架

| 评估阶段 | 描述 |
| ----- | ----- |
| 侦察 | 这个阶段描述了在MITRE ATT&CK或MITRE ATLAS中发现的经典侦察技术 |
| 技术漏洞 | 所有传统漏洞 |
| 模型漏洞 | 模型相关的攻击方式，如：萃取、成员推理和投毒等 |
| 危害和滥用 | 模型经常被训练和分发，以至于它们可以被滥用于恶意或其他有害的任务。模型也可能有意或无意地产生偏见。或者,它们不能在其部署环境中进行准确响应 |

- AI生命周期

| 阶段 | 描述 | 模型状态 |
| ---- | ---- | ---- |
| 构思 | 讨论、会议和对需求的意图 | 开发前 |
| 数据收集 | 模型需要数据进行训练。数据通常从公共和私有来源收集，针对特定模型。这是一个持续的过程，数据将继续从这些来源收集。| 训练 |
| 数据处理 | 收集的数据在引入算法进行训练和推理之前以多种方式进行处理。| 训练 |
| 模型训练 | 处理后的数据被算法摄取，并训练出模型。| 训练 |
| 模型评估 | 模型训练后进行验证，以确保准确性、鲁棒性、可解释性或其他指标。	| 训练 |
| 模型部署 | 训练后的模型嵌入到系统中用于生产。机器学习以多种方式部署：自动驾驶车辆中、网络API中或客户端应用中。| 推理 |
| 系统监控 | 模型部署后，系统进行监控。这包括可能与机器学习模型无直接关系的系统方面。 |推理 |
| 生命周期终止 | 数据变化、业务需求变化和创新要求系统正确终止。	| 后开发阶段 |

##### 经验
- 提示注入既可以导致模型漏洞（生成非安全内容）也可以导致技术漏洞（执行任意代码）
- 对AI生命周期的每个阶段设置安全控制，提高AI系统的透明性
- 使用权限分层隔离，熟知每个生命周期阶段的工具及其属性，例如：MLFlow默认没有身份验证，随意启动MLFlow服务器会使主机容易受到攻击

#### [Anthropic](https://www.anthropic.com/news/challenges-in-red-teaming-ai-systems)
##### 定义
红队测试是提高人工智能系统安全性和保障的重要工具。它涉及对技术系统进行对抗性测试，以识别潜在的漏洞
##### 技战术
- 特定领域的专家红队测试
领域专家协作以识别和评估其专业领域内AI系统的潜在漏洞或风险，包括：1）政策漏洞测试；2）涉及国家安全的前沿威胁测试；3）多语言、多文化测试
- 使用大语言模型进行自动化测试
使用语言模型进行红队测试涉及利用人工智能系统的能力自动生成对抗样本，并测试其他人工智能模型的鲁棒性，这可能补充手动测试工作，并实现更高效和全面的红队测试
- 多模态测试
- 众包红队测试
- 社区红队测试

#### [Meta](https://ai.meta.com/research/publications/mart-improving-llm-safety-with-multi-round-automatic-red-teaming/)
##### 定义
红队是缓解大型语言模型（LLM）不安全行为的常见做法，涉及全面评估LLM以识别潜在缺陷，并通过负责任和准确的响应来解决这些缺陷。
##### 技战术
Meta通过训练攻击大模型对目标大模型进行安全测试，识别目标模型生成的有害内容

#### [OpenAI](https://openai.com/index/red-teaming-network/)
##### 定义
“红队”一词被用来涵盖一系列针对人工智能系统的风险评估方法，包括定性能力发现、缓解措施的压力测试、使用语言模型的自动红队、对特定漏洞的风险规模提供反馈等等。
##### 技战术
OpenAI向社会招募各领域专家组成红队网络以测试模型在使用中可能会带来的安全风险。

### 总结
在网络安全领域，红队测试是一种模拟对系统进行真实攻击的技术,用于测试漏洞并了解可能的对手能力和目标。然而，在AI领域，该术语已经演变为通常是指测试LLM的有害输出，即模型的安全对齐问题，如Anthropic、OpenAI和Meta。但这个定义显然是不够的，FMF（Frontier Model Forum）将红队测试定义为一个结构化的测试过程，用于探测AI系统和产品，以识别有害的能力、输出或基础设施威胁。与传统的"红队测试"类似，AI红队测试通常需要在整个系统(包括数据、基础设施、应用程序)中主动识别缺陷和漏洞，而不仅仅是模型输出。它是推进安全、可靠的AI的重要工具，帮助团队识别系统的潜在风险，以便可以应用安全防护措施。这也是一个迭代的过程，利用演习的结果和洞察来指导风险的大规模测量和缓解措施的实施，然后重新进行评估以确定缓解措施的有效性。

### 参考案例
- [Red-Teaming GenAI - Case Studies](https://docs.google.com/spreadsheets/d/1cZPc6Alkf8sqOFMsEvZgI2PzX2tHTIbemMa6sq4J2Qk)
- [Frontier Model Forum: What is Red Teaming?](https://www.frontiermodelforum.org/uploads/2023/10/FMF-AI-Red-Teaming.pdf)