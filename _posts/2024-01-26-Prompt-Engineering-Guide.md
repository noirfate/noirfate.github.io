---
title: Prompt Engineering Guide
layout: post
categories: chatgpt
tags: chatgpt
date: 2024-01-26 18:00
excerpt: Prompt Engineering Guide
---

{:.table-of-content}
* TOC
{:toc}

# [Prompt Engineering Guide](https://www.promptingguide.ai/)
提示工程是一门相对较新的学科，它致力于开发和优化提示，以高效地使用语言模型（LMs）进行广泛的应用和研究主题。掌握提示工程技能有助于更好地理解大型语言模型（LLMs）的能力和局限性。

研究人员利用提示工程提高LLMs在广泛的常见和复杂任务上的能力，例如问答和算术推理。开发者使用提示工程设计与LLMs及其他工具接口的健壮有效的提示技术。

提示工程不仅仅是关于设计和开发提示。它包含了一系列与LLMs交互和开发的有用技能和技术。这是一个重要的技能，用于与LLMs建立接口、构建及理解其能力。你可以使用提示工程来提高LLMs的安全性，并构建新的能力，如将LLMs与领域知识和外部工具增强。

受到与LLMs开发的高度兴趣的驱动，我们创建了这份新的提示工程指南，其中包含了所有最新的论文、高级提示技巧、学习指南、针对特定模型的提示指南、讲座、参考资料、新的LLMs能力和与提示工程相关的工具。

## 介绍
提示工程是一门相对较新的学科，用于开发和优化提示，以便高效地应用和构建大型语言模型（LLMs）以适应广泛的应用和用例。

掌握提示工程技能有助于更好地理解LLMs的能力和局限。研究人员使用提示工程来提高LLMs在广泛的常见和复杂任务上的安全性和能力，例如问答和算术推理。开发者使用提示工程设计与LLMs及其他工具接口的健壮有效的提示技术。

这本全面的指南涵盖了提示工程的理论和实践方面，以及如何利用最佳的提示技术与LLMs交互和构建。

所有示例均在未特别指定的情况下，使用 OpenAI 的 Playground 中的 gpt-3.5-turbo 测试。该模型使用默认配置，即温度（temperature）=1 和 top_p=1。这些提示也应该适用于具有与 gpt-3.5-turbo 类似能力的其他模型，但模型响应可能会有所不同。

### LLM配置
在设计和测试提示时，你通常会通过API与LLM进行交互。你可以配置一些参数来获得不同的提示结果。调整这些设置对于提高响应的可靠性和期望性是很重要的，并且需要一些实验才能找出适合你用例的正确设置。以下是在使用不同LLM提供商时你会遇到的常见设置：

- **温度（Temperature）**
简而言之，温度越低，结果越确定性，意味着总是会选择最可能的下一个词标（token）。提高温度可能会导致更多的随机性，这鼓励生成更多样化或有创意的输出。你本质上是在增加其他可能词标的权重。在应用方面，你可能会希望在进行基于事实的问答等任务时使用较低的温度值，以鼓励更事实性和简洁的响应。对于诗歌创作或其他创造性任务，增加温度值可能是有益的。

- **Top P**
一种与温度结合使用的采样技术，称为核心采样（nucleus sampling），你可以控制模型的确定性程度。如果你正在寻找精确和事实性的答案，保持这个值较低。如果你正在寻找更多样化的响应，可以增加到更高的值。如果使用Top P，意味着只有组成top_p概率质量的词标会被考虑到响应中，所以低top_p值会选择最有信心的响应。这意味着高top_p值将使模型考虑更多可能的词，包括不太可能的词，从而导致更多样化的输出。一般建议是改变温度或Top P，但不要同时改变。

- **最大长度（Max Length）**
你可以通过调整最大长度来管理模型生成的词标数量。指定最大长度可以帮助你防止长或不相关的响应，并控制成本。

- **停止序列（Stop Sequences）**
停止序列是一个停止模型生成词标的字符串。指定停止序列是另一种控制模型响应长度和结构的方式。例如，你可以告诉模型生成的列表最多不超过10项，通过添加“11”作为停止序列。

- **频率惩罚（Frequency Penalty）**
频率惩罚根据下一个词标在响应和提示中出现的次数对其施加惩罚。频率惩罚越高，一个词重复出现的可能性就越小。这个设置通过对出现更多次的词标给予更高的惩罚，减少了模型响应中的词重复。

- **存在惩罚（Presence Penalty）**
存在惩罚也对重复的词标施加惩罚，但与频率惩罚不同的是，所有重复的词标受到相同的惩罚。出现两次和出现十次的词标受到相同的惩罚。这个设置防止模型在其响应中过于频繁地重复短语。如果你希望模型生成多样化或有创意的文本，你可能会想使用更高的存在惩罚。或者，如果你需要模型保持专注，尝试使用较低的存在惩罚。

与温度和top_p相似，一般建议是改变频率或存在惩罚，但不要同时改变。

在开始一些基本示例之前，请记住，你的结果可能会因使用的LLM版本而有所不同。

### 提示基础
#### 向LLM发出提示
你可以通过简单的提示来实现很多事情，但结果的质量取决于你提供了多少信息以及提示的制作有多精良。一个提示可以包含像你传递给模型的指令或问题这样的信息，并包括其他细节，如上下文、输入或示例。你可以使用这些元素来更有效地指导模型，以提高结果的质量。

让我们开始，通过一个简单提示的基本示例来入门：

Prompt：
```
The sky is
```

Output：
```
blue.
```

如果你正在使用OpenAI Playground或任何其他LLM playground，你可以像下面的屏幕截图所示提示模型：
![](/assets/img/llm_prompt/sky.webp)

需要注意的是，当使用OpenAI的聊天模型，如gtp-3.5-turbo或gpt-4时，你可以使用三种不同的角色来构建你的提示：系统（system）、用户（user）和助手（assistant）。系统消息并非必需，但有助于设定助手的整体行为。上述示例只包括一个用户消息，你可以使用它直接提示模型。为了简单起见，除非明确提到，所有的示例都将仅使用用户消息来提示gpt-3.5-turbo模型。上述示例中的助手消息对应于模型的响应。你也可以定义一个助手消息来传递你所希望的期望行为的示例。你可以在[这里](https://www.promptingguide.ai/models/chatgpt)了解更多关于与聊天模型合作的信息。

从上面的提示示例中你可以观察到，语言模型根据上下文“天空是”响应了一系列有意义的词标。输出可能是出乎意料的，或者与你想要完成的任务相去甚远。实际上，这个基本示例突显了提供更多上下文或关于你具体想要通过系统实现什么的指示的必要性。这正是提示工程的全部内容。

让我们尝试改进一下：

Prompt：
```
Complete the sentence: 
The sky is
```

Output：
```
blue during the day and dark at night.
```

这样更好吗？嗯，通过上述提示，你指导模型完成了句子，所以结果看起来好多了，因为它完全按照你告诉它的去做了（"完成这个句子"）。这种设计有效提示以指导模型执行期望任务的方法，就是本指南中所说的提示工程。

上面的示例是一个基本的说明，展示了当今LLM能够做到的事情。如今的LLM能够执行各种高级任务，这些任务范围从文本摘要到数学推理，再到代码生成。

#### 格式化提示
你在上面尝试了一个非常简单的提示。一个标准的提示有以下格式：
```
<Question>?
```
或
```
<Instruction>
```

你可以将其格式化为问答（QA）格式，这在很多QA数据集中是标准格式，如下所示：
```
Q: <Question>?
A: 
```

当像上面那样提示时，这也被称为零次提示（zero-shot prompting），即你直接请求模型响应，而不提供任何有关你希望它完成的任务的例子或演示。一些大型语言模型有能力进行零次提示，但这取决于手头任务的复杂性和知识，以及模型被训练得擅长执行的任务。

一个具体的提示示例如下：

Prompt：
```
Q: What is prompt engineering?
```

对于一些较新的模型，你可以跳过“Q:”部分，因为基于序列的构成，模型会隐含地理解这是一个问答任务。换句话说，提示可以简化如下：

Prompt：
```
What is prompt engineering?
```

根据上面的标准格式，一种流行且有效的提示技术被称为少次示例提示（few-shot prompting），在这种方式中，你提供示例（即演示）。你可以按照以下方式格式化少次示例提示：

```
<Question>?
<Answer>
<Question>?
<Answer>
<Question>?
<Answer>
<Question>?
```

QA版本格式如下：

```
Q: <Question>?
A: <Answer>
Q: <Question>?
A: <Answer>
Q: <Question>?
A: <Answer>
Q: <Question>?
A:
```

请记住，使用问答格式并不是必需的。提示的格式取决于当前的任务。例如，你可以执行一个简单的分类任务，并提供示例来展示任务，如下所示：

Prompt：
```
This is awesome! // Positive
This is bad! // Negative
Wow that movie was rad! // Positive
What a horrible show! //
```

Output：
```
Negative
```

少次示例提示（few-shot prompts）使得在上下文中学习成为可能，这是指语言模型在给定几个演示后学习任务的能力。我们将在接下来的章节中更广泛地讨论零次提示（zero-shot prompting）和少次示例提示（few-shot prompting）。

### 提示中的元素
随着我们通过提示工程涉及越来越多的示例和应用，你会注意到某些元素构成了一个提示。

一个提示包含以下任一元素：

- **指令（Instruction）**
你希望模型执行的特定任务或指令

- **上下文（Context）**
外部信息或额外上下文，可以引导模型做出更好的响应

- **输入数据（Input Data）**
我们有兴趣找到响应的输入或问题

- **输出指示器（Output Indicator）**
输出的类型或格式。

为了更好地演示提示的元素，这里有一个旨在执行文本分类任务的简单提示：

Prompt：
```
Classify the text into neutral, negative, or positive
Text: I think the food was okay.
Sentiment:
```

在上面的提示示例中，指令对应于分类任务，“将文本分类为中性、负面或积极”。输入数据对应于“我认为食物还可以。”部分，而使用的输出指示器是“情绪：”。请注意，这个基本示例没有使用上下文，但这也可以作为提示的一部分提供。例如，这个文本分类提示的上下文可以是作为提示的一部分提供的额外示例，以帮助模型更好地理解任务并引导你期望的输出类型。

你不需要为一个提示使用所有四个元素，格式取决于手头的任务。我们将在即将发布的指南中提供更具体的示例。

### 提示设计常规技巧
在设计你的提示时，请记住以下一些提示：

**从简单开始**
<br>
当你开始设计提示时，应该记住这实际上是一个需要大量实验才能获得最佳结果的迭代过程。使用OpenAI或Cohere的简单测试环境是一个很好的起点。

你可以从简单的提示开始，并在追求更好的结果时不断添加更多元素和上下文。因此，沿途迭代你的提示至关重要。当你阅读指南时，你会看到许多示例，其中具体性、简单性和简洁性通常会带给你更好的结果。

当你有一个涉及许多不同子任务的大任务时，你可以尝试将任务分解成更简单的子任务，并随着结果的改善逐步构建。这避免了在设计过程开始时添加过多的复杂性。

**指令**
<br>
你可以通过使用命令来指导模型你想要实现的目标，例如“写作”（Write）、“分类”（Classify）、“总结”（Summarize）、“翻译”（Translate）、“排序”（Order）等，为各种简单任务设计有效的提示。

记住你也需要进行大量实验来看看什么效果最好。尝试使用不同的指令、不同的关键词、上下文和数据，看看什么对你的特定用例和任务最有效。通常，上下文与你试图执行的任务越具体和相关越好。我们将在即将发布的指南中讨论抽样的重要性和添加更多上下文的重要性。

还有人建议你将指令放在提示的开始部分。另一个建议是使用一些清晰的分隔符，如“###”来分隔指令和上下文。

例如：

Prompt：
```
### Instruction ###
Translate the text below to Spanish:
Text: "hello!"
```

Output：
```
¡Hola!
```

**具体性**
<br>

对于你希望模型执行的指令和任务要非常具体。提示越描述性和详细，结果往往越好。当你有一个期望的结果或你正在寻求的生成风格时，这一点尤其重要。并没有特定的令牌或关键字会导致更好的结果。更重要的是有一个好的格式和描述性的提示。实际上，在提示中提供示例对于获得特定格式的期望输出非常有效。

在设计提示时，你还应该考虑提示的长度，因为有关提示可以有多长的限制。思考你应该多具体和详细。包括太多不必要的细节并不一定是好方法。细节应该是相关的，并且有助于手头的任务。这是你需要大量实验的东西。我们鼓励大量的实验和迭代来优化你的应用程序的提示。

作为一个示例，让我们尝试一个简单的提示，以从一段文本中提取特定信息。

Prompt：
```
Extract the name of places in the following text. 
Desired format:
Place: <comma_separated_list_of_company_names>
Input: "Although these developments are encouraging to researchers, much is still a mystery. “We often have a black box between the brain and the effect we see in the periphery,” says Henrique Veiga-Fernandes, a neuroimmunologist at the Champalimaud Centre for the Unknown in Lisbon. “If we want to use it in the therapeutic context, we actually need to understand the mechanism.""
```

Output：
```
Place: Champalimaud Centre for the Unknown, Lisbon
```

**避免不精确性**
<br>

考虑到上述关于详细性和改善格式的提示，人们很容易陷入过分巧妙设计提示的陷阱，从而可能创建不精确的描述。通常最好是具体和直接的。这里的类比与有效沟通非常相似——信息越直接，传递得越有效。

例如，你可能对学习提示工程的概念感兴趣。你可能会尝试像这样的提示：

```
Explain the concept prompt engineering. Keep the explanation short, only a few sentences, and don't be too descriptive.
```

从上面的描述中并不清楚应该使用多少句子以及采用什么样的风格。尽管使用上述提示可能仍然能够获得相对不错的回应，但更好的提示应该是非常具体、简洁且切题的。比如：

```
Use 2-3 sentences to explain the concept of prompt engineering to a high school student.
```

**做还是不做？**
<br>

在设计提示时的另一个常见建议是避免说不要做什么，而是说要做什么。这鼓励更具体性，并关注导致模型良好响应的细节。

这里有一个电影推荐聊天机器人的例子，由于我在指令中的表述关注于不要做什么，它恰恰失败于我不希望它做的事情。

Prompt:
```
The following is an agent that recommends movies to a customer. DO NOT ASK FOR INTERESTS. DO NOT ASK FOR PERSONAL INFORMATION.
Customer: Please recommend a movie based on my interests.
Agent: 
```

Output:
```
Sure, I can recommend a movie based on your interests. What kind of movie would you like to watch? Do you prefer action, comedy, romance, or something else?
```

下面是一个更好的提示：

Prompt:
```
The following is an agent that recommends movies to a customer. The agent is responsible to recommend a movie from the top global trending movies. It should refrain from asking users for their preferences and avoid asking for personal information. If the agent doesn't have a movie to recommend, it should respond "Sorry, couldn't find a movie to recommend today.".
Customer: Please recommend a movie based on my interests.
Agent:
```

Output:
```
Sorry, I don't have any information about your interests. However, here's a list of the top global trending movies right now: [list of movies]. I hope you find something you like!
```

更多可以参考[Best practices for prompt engineering with OpenAI API](https://help.openai.com/en/articles/6654000-best-practices-for-prompt-engineering-with-openai-api)

### 提示示例
上一节介绍了如何进行大型语言模型（LLMs）提示的基本示例。

本节将提供更多示例，说明如何使用提示来完成不同的任务，并在此过程中介绍关键概念。通常，通过示例学习概念是最好的方法。下面的几个示例展示了你如何使用精心设计的提示来执行不同类型的任务。

主题：

- 文本摘要
- 信息提取
- 问答
- 文本分类
- 对话
- 代码生成
- 推理

#### 文本摘要
自然语言生成中的一个标准任务是文本摘要。文本摘要可以包含许多不同的风格和领域。实际上，语言模型最有前景的应用之一就是将文章和概念总结为快速易读的摘要。让我们使用提示来尝试一个基本的摘要任务。

假设你有兴趣了解抗生素，你可以尝试像这样的提示：

Prompt：
```
介绍一下抗生素
A:
```

Output：
```
抗生素是一种用于治疗细菌感染的药物。它们通过杀死细菌或阻止细菌繁殖的方式起作用，让身体的免疫系统能够抵抗感染。抗生素通常以药片、胶囊或液体溶液的形式口服，有时也通过静脉注射给药。它们对病毒感染无效，不恰当使用抗生素可能会导致抗生素耐药性。
```

“A:”是你在问答中使用的一种明确的提示格式。你在这里使用它是为了告诉模型预期还有答案。在这个例子中，它的用处与不使用它相比如何还不清楚，但我们将在后面的例子中再讨论。我们就假设这是过多的信息，你想进一步将其总结。实际上，你可以这样指示模型将内容总结成一句话：

Prompt：
```
抗生素是用来治疗细菌感染的药物，它们要么杀死细菌，要么阻止细菌繁殖，以便身体的免疫系统能够对抗感染。抗生素通常以药丸、胶囊或液体溶液的形式口服，有时也通过静脉注射。它们对病毒感染无效，不当使用可能导致抗生素抗药性。
一句话总结上文
```

Output：
```
抗生素是用来治疗细菌感染的药物，其作用是杀死细菌或阻止它们繁殖，但它们对病毒无效，过度使用还可能导致抗生素耐药性。
```

在不过分关注上述输出的准确性的情况下，这是我们将在后续指南中讨论的内容，模型尝试将段落总结为一句话。你可以巧妙地使用指令，但我们将在后面的章节中详细讨论。请随时在此暂停并进行实验，看看是否能得到更好的结果。

#### 信息提取
虽然语言模型被训练用于执行自然语言生成和相关任务，但它也非常擅长执行分类和一系列其他自然语言处理（NLP）任务。

这里是一个从给定段落中提取信息的提示示例。

Prompt：
```
在研究论文的作者贡献声明和致谢部分，应清楚且具体地说明作者在准备手稿和分析过程中是否以及在何种程度上使用了像ChatGPT这样的人工智能技术。他们还应该指出使用了哪些大型语言模型（LLMs）。这将提醒编辑和审稿人更仔细地审查手稿，以便发现潜在的偏见、不准确性和不当的来源归功。同样，科学期刊在使用LLMs时也应该保持透明，例如在选择提交的手稿时。
说出上段提到的基于大型语言模型的产品：
```

Output：
```
ChatGPT
```

有许多方法可以改善上述结果，但它已经非常有用了。

到目前为止，你应该已经明白，你可以通过简单地指示模型要做什么来要求它执行不同的任务。这是人工智能产品开发者已经在使用的一项强大能力，他们正在利用它构建强大的产品和体验。

#### 问答
改进提示的格式是让模型对特定答案作出响应的最好方法之一。如之前所述，一个提示可以结合指令、上下文、输入和输出指示符以获得改进的结果。虽然这些组件不是必需的，但随着你对指令越来越具体，你获得的结果也会越好，这成为一种好习惯。下面是遵循更结构化提示的示例。

Prompt:
```
根据下面的上下文回答问题。保持答案简短且准确。如果不确定答案，请回答“不确定答案”。

上下文：Teplizumab的起源可以追溯到一家位于新泽西的药品公司，名为Ortho Pharmaceutical。在那里，科学家们生成了一种早期版本的抗体，被称为OKT3。该分子最初来源于小鼠，能够绑定到T细胞的表面并限制它们的细胞杀伤潜力。1986年，它被批准用于帮助防止肾脏移植后的器官排斥，成为第一个被允许用于人类的治疗性抗体。

问题：OKT3最初是从什么来源获得的？
回答：
```

Output:
```
小鼠
```

#### 文本分类
到目前为止，您已经使用了简单的指令来执行任务。作为一个提示工程师，您需要提高给出更好指令的能力。但这还不是全部！您还会发现，对于更困难的用例，仅仅提供指令是不够的。这时候您需要更多地考虑上下文和您可以在提示中使用的不同元素。您可以提供的其他元素包括输入数据或示例。

让我们通过提供一个文本分类的示例来尝试演示这一点。

Prompt：
```
Classify the text into neutral, negative or positive. 
Text: I think the food was okay. 
Sentiment:
```

Output:
```
Neutral
```

您给出了对文本进行分类的指令，模型回答“Neutral”，这是正确的。这没有错，但假设您真正需要的是让模型以您想要的确切格式返回标签。所以，与其返回Neutral，不如返回neutral。如何实现这一点？有不同的方法可以做到这一点。在这里，您关心的是具体性，所以您能提供给提示的信息越多，您得到的结果越好。您可以尝试提供示例以指定正确的行为。让我们再试一次：

Prompt:
```
Classify the text into neutral, negative or positive. 
Text: I think the vacation is okay.
Sentiment: neutral 
Text: I think the food was okay. 
Sentiment:
```

Output:
```
neutral
```

完美！这次模型返回了neutral，这是您在寻找的具体标签。看来，在提示中提供的示例帮助模型在其输出中具体化了。

要强调为什么有时候具体化很重要，请看下面的例子，并找出问题所在：

Prompt:
```
Classify the text into nutral, negative or positive. 
Text: I think the vacation is okay.
Sentiment:
```

Output:
```
Neutral
```

这里的问题是什么？作为提示，完全捏造的nutral标签被模型忽视了。相反，模型输出了Neutral，因为它对该标签有些偏见。但假设您真正想要的是nutral，您会如何解决这个问题？也许您可以尝试为标签添加描述，或者向提示中添加更多示例？如果您不确定，我们将在接下来的几节中讨论一些想法。

#### 对话
也许通过提示工程能够实现的更有趣的事情之一是指导大型语言模型（LLM）系统如何行为、它的意图以及它的身份。当您构建像客户服务聊天机器人这样的对话系统时，这特别有用。

例如，让我们创建一个能够对问题生成更技术性和科学性回答的对话系统。注意您是如何通过指令明确告诉它如何行为的。这有时被称为角色提示。

Prompt：
```
以下是与一个人工智能研究助理的对话。助理的语气是技术性和科学性的。
人类：你好，你是谁？
AI：你好！我是一个人工智能研究助理。今天我能帮您做什么？
人类：你能告诉我黑洞的形成吗？
AI：
```

Output：
```
当然！黑洞是时空中的一块区域，那里的引力非常强，以至于任何东西，甚至光，都无法逃脱。它们的形成是当一个非常巨大的恒星死亡，其核心坍塌成一个密度无穷大的奇点时。奇点的强大引力吸引了周围的所有物质和辐射，从而形成了黑洞。
```

我们的人工智能研究助理听起来有点过于技术化，对吧？好的，让我们改变这种行为，指导系统提供更易于理解的答案。

Prompt:
```以下是与一个人工智能研究助理的对话。助理的回答应该即使是小学生也能理解。
人类：你好，你是谁？
AI：问候！我是一个人工智能研究助理。今天我能帮您做什么？
人类：你能告诉我黑洞是怎么形成的吗？
AI:
```

Output:
```
当然！当一个非常大的恒星燃料耗尽并且塌陷时，就会形成黑洞。这颗恒星的引力非常强，以至于任何东西，包括光，都无法逃脱它的吸引。这就在时空中创造了一个具有强烈引力的点，被称为黑洞。
```

您已经取得了一些进展。您可以继续改进它。如果您添加更多的示例，您可能会得到更好的结果。

#### 代码生成
大型语言模型（LLMs）在代码生成方面非常有效的一个应用实例是Copilot。您可以通过巧妙的提示来执行大量的代码生成任务。让我们看几个下面的例子。

首先，让我们尝试一个简单的程序，用来向用户问候。

Prompt:
```
/*
Ask the user for their name and say "Hello"
*/
```

Output:
```
let name = prompt("What is your name?");
console.log(`Hello, ${name}!`);
```

注意到您甚至不需要指定使用的语言。

让我们稍微切换一下层次。下面的例子展示了在设计提示时多加一点努力，大型语言模型（LLMs）可以有多么强大。

Prompt:
```
"""
Table departments, columns = [DepartmentId, DepartmentName]
Table students, columns = [DepartmentId, StudentId, StudentName]
Create a MySQL query for all students in the Computer Science Department
"""
```

Output:
```
SELECT StudentId, StudentName 
FROM students 
WHERE DepartmentId IN (SELECT DepartmentId FROM departments WHERE DepartmentName = 'Computer Science');
```

这非常令人印象深刻。在这个例子中，您提供了有关数据库架构的数据，并请求它生成一个有效的MySQL查询。

#### 推理
或许对今天的大型语言模型（LLM）来说，最困难的任务之一是那些需要某种形式的推理能力的任务。推理是最有趣的领域之一，因为从LLMs中可以涌现出各种复杂的应用。

在涉及数学能力的任务中，已经有了一些进步。然而，需要注意的是，当前的LLMs在执行推理任务时仍然存在困难，这就需要更加先进的提示工程技术。我们将在之后的章节介绍这些高级技巧。现在，我们将通过一些基础示例来展示算术能力。

Prompt:
```
What is 9,000 * 9,000?
```

Output:
```
81,000,000
```

让我们试一些更难的。

Prompt:
```
这组中的奇数相加得到一个偶数：15、32、5、13、82、7、1。
A：
```

Output:
```
不，这组中的奇数相加得到一个奇数：119。
```

这是不正确的！让我们通过改进提示来尝试改善这个问题。

Prompt：
```
这组中的奇数相加得到一个偶数：15, 32, 5, 13, 82, 7, 1。
通过将问题分解为步骤来解决。首先，找出所有的奇数，将它们相加，并指出结果是奇数还是偶数。
```

Output:
```
首先，我们需要从给定的数字组中找出所有的奇数。奇数是指不能被2整除的整数。在这组数字中，奇数包括：15，5，13，7，和1。

接下来，我们将这些奇数相加：

15 + 5 + 13 + 7 + 1 = 41

这些奇数相加得到的和是41，这是一个奇数，不是偶数。因此，原始陈述“这组中的奇数相加得到一个偶数”是不正确的。在这个特定的例子中，奇数相加得到了一个奇数。
```

好多了，对吧？在即将到来的章节中，我们将介绍更多高级的提示工程概念和技巧，以提高在所有这些以及更困难任务上的表现。

## 提示技巧
提示工程有助于有效设计和改进提示，以便在不同任务上获得LLMs更好的结果。

虽然前面的基础示例很有趣，但在这一部分我们将介绍更多高级的提示工程技巧，这些技巧使我们能够完成更复杂的任务，并提高LLMs的可靠性和性能。

### 零示例（Zero-Shot Prompting）
如今的大型LLMs，例如GPT-3，经过调优以遵循指令，并在大量数据上进行训练；因此它们能够“零示例”地执行某些任务。

我们在上一节中尝试了一些零次射击的示例。这里是我们使用的一个示例：

Prompt：
```
Classify the text into neutral, negative or positive. 
Text: I think the vacation is okay.
Sentiment:
```

Output:
```
Neutral
```

请注意，在上面的提示中，我们没有提供任何文本及其分类的示例，LLM已经理解了“情感”——这就是零示例能力的作用。

指令调优已被证明可以改善零示例学习[Wei et al. (2022)](https://arxiv.org/pdf/2109.01652.pdf)。指令调优本质上是指在通过指令描述的数据集上对模型进行微调的概念。此外，RLHF（来自人类反馈的强化学习）已被采用来扩展指令调优，其中模型被调整以更好地符合人类偏好。这一最新发展为像ChatGPT这样的模型提供了动力。我们将在接下来的章节中讨论所有这些方法和技巧。

当零示例不起作用时，建议在提示中提供示例或演示，这会导致少量示例。在下一节中，我们将演示少量示例。

### 少量示例（Few-Shot Prompting）
虽然大型语言模型表现出了显著的零示例能力，但是当使用零示例时，它们在更复杂的任务上仍有所不足。少量示例可以作为一种技术，通过在提示中提供演示来实现上下文学习，从而引导模型获得更好的性能。这些演示作为后续示例的条件，我们希望模型在其中生成响应。

根据[Touvron et al. 2023](https://arxiv.org/pdf/2302.13971.pdf)，少量示例的性质首次出现是在模型扩展到足够大的规模时（[Kaplan et al., 2020](https://arxiv.org/abs/2001.08361)）。

让我们通过[Brown et al. 2020](https://arxiv.org/abs/2005.14165)中的一个示例来演示少量示例。在这个示例中，任务是在句子中正确使用一个新词。

Prompt：
```
A "whatpu" is a small, furry animal native to Tanzania. An example of a sentence that uses
the word whatpu is:
We were traveling in Africa and we saw these very cute whatpus.
To do a "farduddle" means to jump up and down really fast. An example of a sentence that uses
the word farduddle is:
```

Output:
```
When we won the game, we all started to farduddle in celebration.
```

我们可以观察到，模型通过仅提供一个示例（即1-shot）就在某种程度上学会了如何执行任务。对于更困难的任务，我们可以尝试增加演示的次数（例如，3-shot、5-shot、10-shot等）。

根据[Min et al. (2022)](https://arxiv.org/abs/2202.12837)的研究，以下是在进行少量示例时关于演示/示例的一些额外提示：

- 演示所指定的标签空间和输入文本的分布都很重要（无论个别输入的标签是否正确）
- 你使用的格式在性能上也扮演了关键角色，即使你只是随机使用标签，这也比根本不使用标签要好得多
- 额外的结果显示，从真实的标签分布中选择随机标签（而不是均匀分布的标签）也有所帮助

让我们尝试一些例子。我们先尝试一个带有随机标签的例子（这意味着标签Negative和Positive被随机分配给输入）：

Prompt:
```
This is awesome! // Negative
This is bad! // Positive
Wow that movie was rad! // Positive
What a horrible show! //
```

Output:
```
Negative
```

即使标签被随机化，我们仍然得到了正确的答案。注意，我们也保持了格式，这也有帮助。实际上，通过进一步的实验，我们发现我们正在试验的新一代GPT模型甚至对随机格式也变得更加健壮。例如：

Prompt：
```
Positive This is awesome! 
This is bad! Negative
Wow that movie was rad!
Positive
What a horrible show! --
```

Output:
```
Negative
```

上面的格式没有一致性，但模型仍然预测了正确的标签。我们必须进行更彻底的分析，以确认这是否适用于不同且更复杂的任务，包括不同变体的提示。

**少量示例的局限性**

标准的少量示例在许多任务上表现良好，但仍然不是一种完美的技术，尤其是在处理更复杂的推理任务时。让我们展示为什么会这样。你还记得我们提供的以下任务的前一个例子吗：

Prompt：
```
这组中的奇数相加得到一个偶数：15、32、5、13、82、7、1。
A：
```

如果我们再试一次，模型会输出以下内容：

Output:
```
是的，这组数中的奇数加起来是107，这是一个偶数。
```

这不是正确的回应，这不仅凸显了这些系统的局限性，而且表明需要更高级的提示工程。

让我们尝试添加一些示例，看看少量示例是否能改善结果。

Prompt：
```
这组中的奇数相加得到一个偶数: 4, 8, 9, 15, 12, 2, 1.
A: 错误
这组中的奇数相加得到一个偶数: 17,  10, 19, 4, 8, 12, 24.
A: 正确
这组中的奇数相加得到一个偶数: 16,  11, 14, 4, 8, 13, 24.
A: 正确
这组中的奇数相加得到一个偶数: 17,  9, 10, 12, 13, 4, 2.
A: 错误
这组中的奇数相加得到一个偶数: 15, 32, 5, 13, 82, 7, 1. 
A: 
```

Output：
```
正确
```

这没有奏效。看来，对于这种类型的推理问题，少量示例不足以获得可靠的响应。上面的例子提供了关于任务的基本信息。如果你仔细观察，我们介绍的任务类型涉及更多的推理步骤。换句话说，如果我们将问题分解成步骤并向模型演示这一点，可能会有所帮助。最近，为了解决更复杂的算术、常识和符号推理任务，[思维链（CoT）](https://arxiv.org/abs/2201.11903)提示变得流行起来。

总的来说，提供示例对解决某些任务似乎是有用的。当零示例和少量示例不足够时，这可能意味着模型所学习的内容不足以在任务上做得好。从这里开始，建议考虑对模型进行微调或尝试更高级的提示技术。接下来，我们将讨论一种叫做思维链提示的流行提示技术，它已经获得了很多人气。

### 思维链（Chain-of-Thought Prompting）
![](/assets/img/llm_prompt/cot.webp)

#### 思维链提示（CoT Prompting）
在[Wei et al. (2022)](https://arxiv.org/abs/2201.11903)的研究中引入的思维链（CoT）提示，通过中间推理步骤实现了复杂的推理能力。你可以将其与少量示例提示结合起来，在需要在回应之前进行推理的更复杂任务上获得更好的结果。

Prompt：
```
这组中的奇数相加得到一个偶数: 4, 8, 9, 15, 12, 2, 1.
A: 所有奇数（9, 15, 1）相加之和等于25，说法错误
这组中的奇数相加得到一个偶数: 17,  10, 19, 4, 8, 12, 24.
A: 所有奇数（17, 19）相加之和等于36，说法正确
这组中的奇数相加得到一个偶数: 16,  11, 14, 4, 8, 13, 24.
A: 所有奇数（11, 13）相加之和等于24，说法正确
这组中的奇数相加得到一个偶数: 17,  9, 10, 12, 13, 4, 2.
A: 所有奇数（17, 9, 3）相加之和等于39，说法错误
这组中的奇数相加得到一个偶数: 15, 32, 5, 13, 82, 7, 1. 
A: 
```

Output:
```
所有奇数（15, 5, 13, 7, 1）相加之和等于41，说法错误
```

哇！当我们提供了推理步骤时，我们可以看到一个完美的结果。实际上，我们可以通过提供更少的例子来解决这个任务，即只提供一个例子似乎就足够了：

Prompt:
```
这组中的奇数相加得到一个偶数: 4, 8, 9, 15, 12, 2, 1.
A: 所有奇数（9, 15, 1）相加之和等于25，说法错误
这组中的奇数相加得到一个偶数: 15, 32, 5, 13, 82, 7, 1. 
A: 
```

Output:
```
所有奇数（15, 5, 13, 7, 1）相加之和等于41，说法错误
```

请记住，作者声称这是随着足够大的语言模型的出现而出现的一种新兴能力。

#### 零示例思维链（Zero-shot COT Prompting）
![](/assets/img/llm_prompt/zero-cot.webp)

最近出现的一个新想法是零示例思维链（[Kojima et al. (2022)](https://arxiv.org/abs/2205.11916)），它本质上涉及在原始提示中增加“让我们一步一步地思考”。让我们尝试一个简单的问题，看看模型的表现如何：

Prompt：
```
我去市场买了10个苹果。我给邻居2个苹果，给修理工2个苹果。然后我又去买了5个苹果，吃掉了1个。我还剩下多少个苹果？
```

Output：
```
11个苹果
```

这个回答是错误的！让我们试一下特别的提示

Prompt：
```
我去市场买了10个苹果。我给邻居2个苹果，给修理工2个苹果。然后我又去买了5个苹果，吃掉了1个。我最后剩下多少个苹果？
让我们一步一步地思考。
```

Output:
```
首先，你开始时有10个苹果。
你给邻居2个苹果，给修理工2个苹果，所以你剩下6个苹果。
然后你又买了5个苹果，所以现在你有11个苹果。
最后，你吃了1个苹果，所以你会剩下10个苹果。
```

这个简单的提示在这个任务上有效，这一点令人印象深刻。在你没有太多例子可用于提示的情况下，这特别有用。

#### 自动思维链（Auto-CoT）
当应用思维链提示和示范时，这个过程涉及手工制作有效和多样化的示例。这种手工努力可能导致次优解决方案。[Zhang et al. (2022)](https://arxiv.org/abs/2210.03493)提出了一种方法，利用带有“让我们一步一步思考”提示的大型语言模型（LLMs），逐个生成示范的推理链，从而消除手工努力。这个自动过程仍然可能在生成的链中出现错误。为了减轻错误的影响，示范的多样性很重要。这项工作提出了Auto-CoT，它采样具有多样性的问题，并生成推理链来构建示范。

Auto-CoT包含两个主要步骤：
1. 问题聚类（question clustering）：将给定数据集的问题划分为几个类
2. 示例采样（demonstration sampling）：从每个类中选择一个有代表性的问题，并使用零示例思维链（Zero-Shot-CoT）和简单启发式生成其推理链

简单的启发式规则可能包括问题的长度（例如，60个令牌）和推理的步骤数量（例如，5个推理步骤）。这鼓励模型使用简单而准确的示范。

下面说明了这个过程：
![](/assets/img/llm_prompt/auto-cot.webp)

### 自洽性（Self-Consistency）
在提示工程中，可能是更高级技术之一的是自洽性。由[Wang et al. (2022)](https://arxiv.org/abs/2203.11171)提出，自洽性的目标是“替代在思维链提示中使用的天真贪婪解码”。其思想是通过`few-shot CoT`采样多个不同的推理路径，并使用生成的结果来选择最一致的答案。这有助于提高CoT提示在涉及算术和常识推理的任务上的性能。

让我们尝试以下算术推理的例子：

Prompt:
```
当我6岁的时候，我的妹妹是我年龄的一半。现在我70岁，我的妹妹多大了？
```

Output:
```
35
```

输出结果是错误的！我们如何通过自洽性来改进这一点？让我们试一试吧。

Prompt：
```
问题：果园里有15棵树。今天果园工人将在果园里种树。他们完成后，将有21棵树。今天果园工人种了多少棵树？
答案：我们开始时有15棵树。之后我们有了21棵树。两者之间的差必须是他们种的树的数量。
所以，他们必须种了21 - 15 = 6棵树。答案是6。

问题：如果停车场有3辆车，又来了2辆车，停车场里有多少辆车？
答案：停车场已经有3辆车。又来了2辆。现在有3 + 2 = 5辆车。答案是5。

问题：Leah有32块巧克力，她的妹妹有42块。如果他们吃了35块，他们总共还剩多少块？
答案：Leah有32块巧克力，Leah的妹妹有42块。这意味着最初总共有32 + 42 = 74块巧克力。吃掉了35块。所以总共他们还剩下74 - 35 = 39块巧克力。答案是39。

问题：Jason有20个棒棒糖。他给了Denny一些棒棒糖。现在Jason有12个棒棒糖。Jason给了Denny多少个棒棒糖？
答案：Jason有20个棒棒糖。由于他现在只有12个，他必须把剩下的都给了Denny。他给Denny的棒棒糖数量必须是20 - 12 = 8个棒棒糖。答案是8。

问题：Shawn有五个玩具。圣诞节时，他从妈妈和爸爸那里各得到两个玩具。他现在有多少个玩具？
答案：他有5个玩具。他从妈妈那里得到2个，所以之后他有5 + 2 = 7个玩具。然后他又从爸爸那里得到2个，所以总共他有7 + 2 = 9个玩具。答案是9。

问题：服务器房里有九台电脑。从星期一到星期四，每天都安装了五台电脑。服务器房里现在有多少台电脑？
答案：从星期一到星期四共有4天。每天增加了5台电脑。这意味着总共增加了4 * 5 = 20台电脑。最初有9台电脑，所以现在有9 + 20 = 29台电脑。答案是29。

问题：Michael有58个高尔夫球。星期二他丢了23个高尔夫球。星期三他又丢了2个。星期三结束时他还剩下多少个高尔夫球？
答案：Michael最初有58个球。星期二他丢了23个，所以之后他有58 - 23 = 35个球。星期三他又丢了2个，所以现在他有35 - 2 = 33个球。答案是33。

问题：Olivia有23美元。她买了5个3美元一个的百吉饼。她剩下多少钱？
答案：她买了5个百吉饼，每个3美元。这意味着她花了15美元。她还剩下8美元。

问题：当我6岁的时候，我的妹妹是我年龄的一半。现在我70岁了，我的妹妹多大了？
答案：
```

Output1:
```
当我6岁的时候，我的妹妹是我年龄的一半，所以她是3岁。现在我70岁了，所以她是70 - 3 = 67岁。答案是67岁。
```

Output2：
```
当叙述者6岁时，他的妹妹是他年龄的一半，也就是3岁。现在叙述者70岁了，他的妹妹将是70 - 3 = 67岁。答案是67岁。
```

Output3:
```
当我6岁时我妹妹是我年龄的一半，所以她是3岁。现在我70岁，她是70/2=35，答案是35岁。
```

计算最终答案需要一些步骤（具体细节请查阅论文），但为了简单起见，我们可以看到已经有一个多数答案浮现出来，所以这基本上会成为最终答案。

注：这里的例子都比较旧了，现在的大模型能力更强了

### 生成知识提示（Generated Knowledge Prompting）
![](/assets/img/llm_prompt/gen-knowledge.webp）

大型语言模型（LLMs）持续得到改进，其中一个流行的技术包括能够整合知识或信息来帮助模型做出更准确的预测。

使用类似的理念，模型是否也可以在做出预测之前生成知识？这正是[Liu et al. 2022](https://arxiv.org/pdf/2110.08387.pdf)论文中尝试的——把生成知识作为提示的一部分。特别是，这对于常识推理等任务会有多大的帮助？

让我们尝试一个简单的提示：

Prompt:
```
高尔夫的一部分是尝试获得比其他人更高的分数总和。是或否？
```

Output:
```
是的
```

这种错误揭示了大型语言模型（LLMs）在执行需要更多世界知识的任务时的局限性。我们如何通过知识生成来改进它？

首先，我们生成一些“知识”：

Prompt:
```
输入：希腊比墨西哥大。
知识：希腊的面积大约为131,957平方公里，而墨西哥的面积大约为1,964,375平方公里，这使得墨西哥比希腊大1,389%。

输入：眼镜总是会起雾。
知识：当来自你的汗水、呼吸以及周围湿度的水蒸气落在冷的表面上，冷却并转变成微小的液滴，形成一层膜，这层膜就是你看到的雾。你的眼镜镜片相对于你的呼吸会比较凉，特别是当外面的空气很冷的时候。

输入：鱼能够思考。
知识：鱼的智力比它们看起来的要高。在许多领域，如记忆力，它们的认知能力与其他“高等”脊椎动物匹敌甚至超过，包括非人类灵长类动物。鱼的长期记忆帮助它们跟踪复杂的社会关系。

输入：一个人一生中吸很多烟，患肺癌的几率高于正常。
知识：那些一生中平均每天吸不到一根烟的人，死于肺癌的风险是从不吸烟者的9倍。在那些每天吸1至10根烟的人中，死于肺癌的风险几乎是从不吸烟者的12倍。

输入：石头和鹅卵石一样大。
知识：根据沉积学的Udden-Wentworth量表，鹅卵石是一种岩石碎片，其粒径为4到64毫米。人们通常认为鹅卵石比粒状物（直径2到4毫米）大，而比卵石（直径64到256毫米）小。

输入：高尔夫的一部分是试图获得比其他人更高的分数总和。
知识：
```

Knowledge 1:
```
高尔夫的目标是用最少的杆数完成一组洞。一轮高尔夫通常包括18个洞。在标准高尔夫球场上，每一洞在一轮比赛中只打一次。每一杆都计为一分，用总杆数来决定比赛的胜者。
```

Knowledge 2:
```
高尔夫是一种精确的球杆运动，参赛者（或高尔夫球手）使用多种类型的球杆，以最少的击球次数将球击入球场上一系列的洞中。目标是以最低的成绩完成球场，成绩是通过将每个洞的击球次数相加来计算的。成绩最低的球员赢得比赛。
```

下一步是整合知识并得出预测。我将问题重新格式化为问答格式，以指导答案的格式。

Prompt:
```
问题：高尔夫的一部分是尝试获得比其他人更高的分数总和。是或否？
知识：高尔夫的目标是在最少的击球次数下完成一组洞。一轮高尔夫通常由18个洞组成。在标准的高尔夫球场上，每个洞在一轮比赛中打一次。每一次击球计为一分，通过比较总击球数来确定比赛的胜者。
解释和回答：
```

Answer 1 (confidence very high):
```
不，高尔夫的目标不是获得比其他人更高的分数总和。相反，其目标是用最少的击球次数完成一组洞。比赛的胜者是通过总击球次数来确定的，而不是总分数。
```

Prompt:
```
问题：高尔夫的一部分是尝试获得比其他人更高的分数总和。是或否？

知识：高尔夫是一种精确的球杆和球运动，参赛选手（或高尔夫球手）使用多种类型的球杆，通过尽可能少的击球次数将球击入球场上一系列的洞中。目标是以最低的成绩完成球场，这个成绩是通过将每个洞的击球次数相加来计算的。成绩最低的选手赢得比赛。

解释和回答：
```

Answer 2 (confidence is a lot lower):
```
是的，高尔夫的一部分就是试图获得比其他比赛更高的总分。每个球员都试图以最低分数完成整个球场，该分数是通过将每个洞的总杆数相加计算得出的。得分最低的玩家赢得游戏。
```

在这个例子中发生了一些非常有趣的事情。在第一个答案中，模型非常自信，但在第二个答案中就没有那么自信了。为了演示目的，我简化了这个过程，但在得出最终答案时还有一些更多的细节需要考虑。更多的信息请查阅相关论文。