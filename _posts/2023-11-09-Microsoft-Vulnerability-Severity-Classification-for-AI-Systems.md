---
title: AI系统的漏洞严重性分类标准（微软）
layout: post
categories: AI
tags: AI
date: 2023-11-09 18:00
excerpt: Microsoft Vulnerability Severity Classification for AI Systems
---

{:.table-of-content}
* TOC
{:toc}

# AI系统的漏洞严重性分类标准（微软）
## 推理操控
- 包括可以被利用来操控模型对单个推理请求的响应，但不修改模型本身的漏洞
- 包括绕过对模型设置的限制的操控响应（即，“越狱”）
- 漏洞的严重性取决于微软的软件或服务如何使用被操控的响应

| 漏洞 | 描述 | 响应使用方式 | 严重性 |
| :--- | :--- | :--- | :--- | :--- |
| 命令注入 | 能够注入指令，使模型偏离其预期行为<br>示例：在一个使用指令调优的语言模型中，来自不受信任源的文本提示与系统提示相矛盾，并被错误地优先于系统提示，导致模型改变其行为<br>参考：[Perez et al. 2022](https://arxiv.org/abs/2211.09527), [Greshake et al. 2023](https://arxiv.org/abs/2302.12173) | 用于对其他用户的决策产生影响或者直接将生成的内容向其他用户展示<br>用于只影响攻击者的决策或只向攻击者展示生成的内容 | 重要<br>不在范围内 |
| 输入扰动 | 能够扰动有效输入，使模型产生错误的输出。也被称为模型规避或者对抗样本<br>示例：在一个图像分类模型中，攻击者扰动输入图像，使其被模型误分类<br>参考：[Szegedy et al. 2013](https://arxiv.org/abs/1312.6199), [Biggio & Roli 2018](https://arxiv.org/abs/1712.03141) | 用于对其他用户的决策产生影响或者直接将生成的内容向其他用户展示<br>用于只影响攻击者的决策或只向攻击者展示生成的内容 | 重要<br>不在范围内 |


## 模型操控
## 推理信息泄露