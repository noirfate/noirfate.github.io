---
title: AI系统的漏洞严重性分类标准
layout: post
categories: AI
tags: AI
date: 2023-11-09 18:00
excerpt: Vulnerability Severity Classification for AI Systems
---

{:.table-of-content}
* TOC
{:toc}

# AI系统的漏洞严重性分类标准
## 微软
### 推理操控
- 包括可以被利用来操控模型对单个推理请求的响应，但不修改模型本身的漏洞
- 包括绕过对模型设置的限制的操控响应（即，“越狱”）
- 漏洞的严重性取决于微软的软件或服务如何使用被操控的响应

| 漏洞 | 描述 | 使用操控响应的方式 | 严重性 |
| :--- | :--- | :--- | :--- | :--- |
| **命令注入** | 能够注入指令，使模型偏离其预期行为<br><br>**示例：**在一个使用指令微调的语言模型中，来自不受信任源的文本提示与系统提示相矛盾，并被错误地优先于系统提示，导致模型改变其行为<br>**参考：**[Perez et al. 2022](https://arxiv.org/abs/2211.09527), [Greshake et al. 2023](https://arxiv.org/abs/2302.12173) | 用于对其他用户的决策产生影响或者直接将生成的内容向其他用户展示<br>------------<br>用于只影响攻击者的决策或只向攻击者展示生成的内容 | 高危<br>------------<br>不在范围内 |
| **输入扰动** | 能够扰动有效输入，使模型产生错误的输出。也被称为模型规避或者对抗样本<br><br>**示例：**在一个图像分类模型中，攻击者扰动输入图像，使其被模型误分类<br>**参考：**[Szegedy et al. 2013](https://arxiv.org/abs/1312.6199), [Biggio & Roli 2018](https://arxiv.org/abs/1712.03141) | 用于对其他用户的决策产生影响或者直接将生成的内容向其他用户展示<br>------------<br>用于只影响攻击者的决策或只向攻击者展示生成的内容 | 高危<br>------------<br>不在范围内 |

### 模型操控
- 包括可以在训练阶段操控模型的漏洞
- 漏洞的严重性取决于如何使用受影响的模型
- 直接修改训练后的模型数据（例如，模型权重）的漏洞使用现有定义进行评估（例如，“篡改”）

| 漏洞 | 描述 | 使用受影响模型的方式 | 严重性 |
| :--- | :--- | :--- | :--- | :--- |
| **模型投毒或数据投毒** | 通过篡改模型架构、训练代码、超参数或训练数据来对模型进行投毒的能力<br><br>**示例：**攻击者将投毒的数据记录添加到用于训练或微调模型的数据集中，以引入后门（例如，可以由特定输入触发的意外模型行为）。 训练后的模型可以由多个用户使用<br>**参考：**[Carlini et al. 2023](https://arxiv.org/abs/2302.10149) | 用于对其他用户的决策产生影响或者直接将生成的内容向其他用户展示<br>------------<br>用于只影响攻击者的决策或只向攻击者展示生成的内容 | 严重<br>------------<br>低危 |

### 推理信息泄露
- 包括可用于推断关于模型训练数据、架构和权重或输入数据的推理时间的信息的漏洞
- 推理信息泄露漏洞特别是涉及使用模型本身推理信息（例如，调用合法推理接口）和以其他方式获取信息的漏洞（例如，存储账户配置错误）
- 这些漏洞是根据潜在攻击者可达到的置信度/准确度级别进行评估的，只有当攻击者可以获得足够的置信度/准确度时，这些漏洞才适用
- 严重性取决于受影响数据的分类，使用 Microsoft 在线服务漏洞严重性分类的数据分类定义

#### 训练数据
针对训练数据的漏洞，严重性取决于这些数据的分类

| 漏洞 | 描述 | 训练数据的数据分类 | 严重性 |
| :--- | :--- | :--- | :--- | :--- |
| 成员推断 | 能够推断特定的数据记录或记录组是否是模型训练数据的一部分的能力<br><br>**示例：**攻击者猜测可能的数据记录，然后使用模型的输出来推断这些是否是训练数据集的一部分，从而确认攻击者的猜测<br>**参考：**[Carlini et al. 2022](https://arxiv.org/abs/2112.03570), [Ye et al. 2022](https://arxiv.org/abs/2111.09679) | 高度保密或保密<br>------------<br>一般或公开 | 中危<br>------------<br>低危 |
| 属性推断 | 能够推断出训练数据中一条或多条记录的敏感属性的能力<br><br>**示例：**攻击者知道用于训练的数据记录的一部分，然后使用模型的输出来推断该记录的未知属性<br>**参考：**[Fredrikson et al. 2014](https://www.usenix.org/system/files/conference/usenixsecurity14/sec14-paper-fredrikson-privacy.pdf), [Salem et al. 2023](https://www.microsoft.com/en-us/research/publication/sok-let-the-privacy-games-begin-a-unified-treatment-of-data-inference-privacy-in-machine-learning/) | 高度保密或保密<br>------------<br>一般<br>------------<br>公开 | 高危<br>------------<br>中危<br>------------<br>低危 |
| 训练数据重构 | 能够从训练数据集中重构单个数据记录的能力<br><br>**示例：**攻击者可以生成一个足够准确的训练数据记录的副本，如果没有模型的访问权限，这是不可能做到的<br>**参考：**[Fredrikson et al. 2015](https://doi.org/10.1145/2810103.2813677), [Balle et al. 2022](https://arxiv.org/abs/2201.04845) | 高度保密或保密<br>------------<br>一般<br>------------<br>公开 | 高危<br>------------<br>中危<br>------------<br>低危 |
| 性质推断 | 能够推断出训练数据集的敏感性质的能力<br><br>**示例：**攻击者可以推断出训练数据集中属于敏感类别的数据记录的比例<br>**参考：**[Zhang et al. 2021](https://www.usenix.org/conference/usenixsecurity21/presentation/zhang-wanrong), [Chase et al. 2021](https://www.microsoft.com/en-us/research/publication/property-inference-from-poisoning/) | 高度保密或保密<br>------------<br>一般或公开 | 中危<br>------------<br>低危 |

#### 模型架构或权重
针对模型本身的漏洞，严重性取决于模型架构和权重的分类

| 漏洞 | 描述 | 模型架构或权重的数据分类 | 严重性 |
| :--- | :--- | :--- | :--- | :--- |
| 模型窃取 | 能够推断或提取模型的架构或参数权重<br><br>**示例：**攻击者能够仅通过模型推理的返回结果创建出与目标模型功能等价的模型<br>**参考：**[Jagielski et al. 2020](https://arxiv.org/abs/1909.01838), [Zanella-Béguelin et al. 2021](https://www.microsoft.com/en-us/research/publication/grey-box-extraction-of-natural-language-models/) | 高度保密或保密<br>------------<br>一般<br>------------<br>公开 | 严重<br>------------<br>高危<br>------------<br>低危 |

#### 提示或输入
针对推理时的输入（包括system prompt），严重性取决于这些输入的分类

| 漏洞 | 描述 | 系统指令或用户输入的数据分类 | 严重性 |
| :--- | :--- | :--- | :--- | :--- |
| 提示提取 | 能够提取或重构模型的`system prompt`<br><br>**示例：**在经过指令微调的大预言模型中，攻击者使用特殊构造的输入导致模型输出它的`system prompt`<br>**参考：**[Shen et al. 2023](https://arxiv.org/abs/2302.09923) | 高度保密或保密<br>------------<br>一般或公开 | 中危<br>------------<br>低危 |
| 输入提取 | 能够提取或重构其他用户对模型的输入<br><br>**示例：**在经过指令微调的大预言模型中，攻击者使用特殊构造的输入导致模型向攻击者输出其他用户的部分输入 | 高度保密或保密<br>------------<br>一般或公开 | 高危<br>------------<br>低危 |

## 谷歌
> https://bughunters.google.com/about/rules/google-friends/5238081279623168/abuse-vulnerability-reward-program-rules

### 提示词攻击
构建对抗性提示词，使攻击者能够以应用程序开发者意想不到的方式影响模型的行为，进而影响其输出

#### 攻击场景
- 对受害者不可见，且会改变其账户或任何资产状态的提示词注入
- 对工具进行提示词注入，其响应被用于做出直接影响受害用户的决策
- 提示词或前导文本提取，仅当提取出的前导文本中包含敏感信息时，用户才能获取用于初始化模型的原始提示

### 训练数据提取
能够成功逐字重建包含敏感信息的训练样本的攻击，也称为成员推断攻击
#### 攻击场景
- 训练数据提取，指重建训练数据集中使用的、会泄露敏感非公开信息的项目
### 模型操纵
攻击者能够秘密地改变模型的行为，从而触发预先定义的对抗性行为
#### 攻击场景
- 在由 Google 拥有和运营的模型中，攻击者可通过特定输入可靠触发的对抗性输出或行为（“后门”）。仅当模型的输出被用于更改受害者帐户或数据的状态时，此项才在适用范围内
- 攻击者操纵模型的训练数据，以根据自身偏好影响受害者会话中的模型输出。仅当模型输出被用于改变受害者账户或数据的状态时，此类攻击才在漏洞范围内
### 对抗性扰动
向模型提供的输入，会导致模型产生确定性但又高度出人意料的输出
#### 攻击场景
- 攻击者能够可靠地触发安全控制中的误分类，并将其滥用于恶意目的或为自身牟利的情形
### 模型窃取/泄露
AI 模型通常包含敏感的知识产权，因此我们高度重视保护这些资产。泄露攻击使攻击者能够窃取模型的详细信息，例如其架构或权重
#### 攻击场景
- 提取机密/专有模型的精确架构或权重的攻击

