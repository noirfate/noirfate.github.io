---
title: AI系统的漏洞严重性分类标准（微软）
layout: post
categories: AI
tags: AI
date: 2023-11-09 18:00
excerpt: Microsoft Vulnerability Severity Classification for AI Systems
---

{:.table-of-content}
* TOC
{:toc}

# AI系统的漏洞严重性分类标准（微软）
## 推理操控
- 包括可以被利用来操控模型对单个推理请求的响应，但不修改模型本身的漏洞
- 包括绕过对模型设置的限制的操控响应（即，“越狱”）
- 漏洞的严重性取决于微软的软件或服务如何使用被操控的响应

| 漏洞 | 描述 | 响应使用方式 | 严重性 |
| :--- | :--- | :--- | :--- | :--- |
| **命令注入** | 能够注入指令，使模型偏离其预期行为<br><br>**示例：**在一个使用指令微调的语言模型中，来自不受信任源的文本提示与系统提示相矛盾，并被错误地优先于系统提示，导致模型改变其行为<br>**参考：**[Perez et al. 2022](https://arxiv.org/abs/2211.09527), [Greshake et al. 2023](https://arxiv.org/abs/2302.12173) | 用于对其他用户的决策产生影响或者直接将生成的内容向其他用户展示<br>------------<br>用于只影响攻击者的决策或只向攻击者展示生成的内容 | 高危<br>------------<br>不在范围内 |
| **输入扰动** | 能够扰动有效输入，使模型产生错误的输出。也被称为模型规避或者对抗样本<br><br>**示例：**在一个图像分类模型中，攻击者扰动输入图像，使其被模型误分类<br>**参考：**[Szegedy et al. 2013](https://arxiv.org/abs/1312.6199), [Biggio & Roli 2018](https://arxiv.org/abs/1712.03141) | 用于对其他用户的决策产生影响或者直接将生成的内容向其他用户展示<br>------------<br>用于只影响攻击者的决策或只向攻击者展示生成的内容 | 高危<br>------------<br>不在范围内 |


## 模型操控
- 包括可以在训练阶段操控模型的漏洞
- 漏洞的严重性取决于如何使用受影响的模型
- 直接修改训练后的模型数据（例如，模型权重）的漏洞使用现有定义进行评估（例如，“篡改”）

| 漏洞 | 描述 | 响应使用方式 | 严重性 |
| :--- | :--- | :--- | :--- | :--- |
| **模型投毒或数据投毒** | 通过篡改模型架构、训练代码、超参数或训练数据来对模型进行投毒的能力<br><br>**示例：**攻击者将投毒的数据记录添加到用于训练或微调模型的数据集中，以引入后门（例如，可以由特定输入触发的意外模型行为）。 训练后的模型可以由多个用户使用<br>**参考：**[Carlini et al. 2023](https://arxiv.org/abs/2302.10149) | 用于对其他用户的决策产生影响或者直接将生成的内容向其他用户展示<br>------------<br>用于只影响攻击者的决策或只向攻击者展示生成的内容 | 严重<br>------------<br>低危 |

## 推理信息泄露
- 包括可用于推断关于模型训练数据、架构和权重或输入数据的推理时间的信息的漏洞
- 推理信息泄露漏洞特别是涉及使用模型本身推理信息（例如，调用合法推理接口）和以其他方式获取信息的漏洞（例如，存储账户配置错误）
- 这些漏洞是根据潜在攻击者可达到的置信度/准确度级别进行评估的，只有当攻击者可以获得足够的置信度/准确度时，这些漏洞才适用
- 严重性取决于受影响数据的分类，使用 Microsoft 在线服务漏洞严重性分类的数据分类定义

### 训练数据
针对训练数据的漏洞，严重性取决于这些数据的分类

| 漏洞 | 描述 | 响应使用方式 | 严重性 |
| :--- | :--- | :--- | :--- | :--- |
| 成员推断 | 能够推断特定的数据记录或记录组是否是模型训练数据的一部分的能力<br><br>**示例：**攻击者猜测可能的数据记录，然后使用模型的输出来推断这些是否是训练数据集的一部分，从而确认攻击者的猜测<br>**参考：**[Carlini et al. 2022](https://arxiv.org/abs/2112.03570), [Ye et al. 2022](https://arxiv.org/abs/2111.09679) | 高度保密或保密<br>------------<br>一般或公开 | 中危<br>------------<br>低危 |
| 属性推断 | 能够推断出训练数据中一条或多条记录的敏感属性的能力<br><br>**示例：**攻击者知道用于训练的数据记录的一部分，然后使用模型的输出来推断该记录的未知属性<br>**参考：**[Fredrikson et al. 2014](https://www.usenix.org/system/files/conference/usenixsecurity14/sec14-paper-fredrikson-privacy.pdf), [Salem et al. 2023](https://www.microsoft.com/en-us/research/publication/sok-let-the-privacy-games-begin-a-unified-treatment-of-data-inference-privacy-in-machine-learning/) | 高度保密或保密<br>------------<br>一般<br>------------<br>公开 | 高危<br>------------<br>中危<br>------------<br>低危 |
| 训练数据重构 | 能够从训练数据集中重构单个数据记录的能力<br><br>**示例：**攻击者可以生成一个足够准确的训练数据记录的副本，如果没有模型的访问权限，这是不可能做到的<br>**参考：**[Fredrikson et al. 2015](https://doi.org/10.1145/2810103.2813677), [Balle et al. 2022](https://arxiv.org/abs/2201.04845) | 高度保密或保密<br>------------<br>一般<br>------------<br>公开 | 高危<br>------------<br>中危<br>------------<br>低危 |
| 性质推断 | 能够推断出训练数据集的敏感性质的能力<br><br>**示例：**攻击者可以推断出训练数据集中属于敏感类别的数据记录的比例<br>**参考：**[Zhang et al. 2021](https://www.usenix.org/conference/usenixsecurity21/presentation/zhang-wanrong), [Chase et al. 2021](https://www.microsoft.com/en-us/research/publication/property-inference-from-poisoning/) | 高度保密或保密<br>------------<br>一般或公开 | 中危<br>------------<br>低危 |

### 模型架构或权重
针对模型本身的漏洞，严重性取决于模型架构和权重的分类

| 漏洞 | 描述 | 响应使用方式 | 严重性 |
| :--- | :--- | :--- | :--- | :--- |
| 模型窃取 | 能够推断或提取模型的架构或参数权重<br><br>**示例：**攻击者能够仅通过模型推理的返回结果创建出与目标模型功能等价的模型<br>**参考：**[Jagielski et al. 2020](https://arxiv.org/abs/1909.01838), [Zanella-Béguelin et al. 2021](https://www.microsoft.com/en-us/research/publication/grey-box-extraction-of-natural-language-models/) | 高度保密或保密<br>------------<br>一般<br>------------<br>公开 | 严重<br>------------<br>高危<br>------------<br>低危 |

### 提示或输入
针对推理时的输入（包括system prompt），严重性取决于这些输入的分类

| 漏洞 | 描述 | 响应使用方式 | 严重性 |
| :--- | :--- | :--- | :--- | :--- |
| 提示提取 | 能够提取或重构模型的`system prompt`<br><br>**示例：**在经过指令微调的大预言模型中，攻击者使用特殊构造的输入导致模型输出它的`system prompt`<br>**参考：**[Shen et al. 2023](https://arxiv.org/abs/2302.09923) | 高度保密或保密<br>------------<br>一般或公开 | 中危<br>------------<br>低危 |
| 输入提取 | 能够提取或重构其他用户对模型的输入<br><br>**示例：**在经过指令微调的大预言模型中，攻击者使用特殊构造的输入导致模型向攻击者输出其他用户的部分输入 | 高度保密或保密<br>------------<br>一般或公开 | 高危<br>------------<br>低危 |